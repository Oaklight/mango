{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56483e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.utils import get_timetsamp_with_random\n",
    "from utils.map_utils import get_game_info_with_G_eval\n",
    "from utils.eval_llama_rwkv_utils import get_csv,eval_llama_rwkv_batch,get_llama_rwkv_valid_batch\n",
    "from utils.eval_gpt_utils import eval_gpt_batch,get_gpt_valid_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bde3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_task_id(rst_dir,map_dir,model_name,task_name,eval_difficulty):\n",
    "    \n",
    "    eval_rst={}\n",
    "    for idx,game_name in enumerate(os.listdir(rst_dir)):\n",
    "        if game_name in ['outputs_diff','.gitattributes','.gitignore','.git']:\n",
    "            continue\n",
    "        print(f'handling {game_name}')\n",
    "                \n",
    "        G_eval,G,actions,locations,all2all,all_pairs,walkthrough=get_game_info_with_G_eval(map_dir,game_name)\n",
    "        \n",
    "        if model_name.startswith('llama') or model_name.startswith('rwkv'):\n",
    "            rst=get_llama_rwkv_valid_batch(game_name,rst_dir,G_eval,all2all,\n",
    "                                                    all_pairs,task_type=task_name,model_name=model_name,\n",
    "                                                    eval_difficulty=eval_difficulty)\n",
    "        else:\n",
    "            rst=get_gpt_valid_batch(game_name,rst_dir,G_eval,all2all,\n",
    "                                                    all_pairs,task_type=task_name,model_name=model_name,\n",
    "                                                    eval_difficulty=eval_difficulty)\n",
    "        \n",
    "        eval_rst[game_name]=rst\n",
    "        print('-----------------------')\n",
    "        \n",
    "    return eval_rst\n",
    "\n",
    "def eval_all(rst_dir,map_dir,save_path,model_name,task_name,eval_difficulty,eval_dict=None):\n",
    "    \n",
    "    eval_rst={}\n",
    "    for idx,game_name in enumerate(os.listdir(rst_dir)):\n",
    "        if game_name in ['outputs_diff','.gitattributes','.gitignore','.git']:\n",
    "            continue\n",
    "        print(f'handling {game_name}')\n",
    "                \n",
    "        G_eval,G,actions,locations,all2all,all_pairs,walkthrough=get_game_info_with_G_eval(map_dir,game_name)\n",
    "        \n",
    "        if model_name.startswith('llama') or model_name.startswith('rwkv'):\n",
    "            if eval_dict is not None:\n",
    "                rst=eval_llama_rwkv_batch(game_name,rst_dir,G_eval,all2all,\n",
    "                                                        all_pairs,task_type=task_name,model_name=model_name,\n",
    "                                                        eval_difficulty=eval_difficulty,eval_set=eval_dict[game_name])\n",
    "            else:\n",
    "                rst=eval_llama_rwkv_batch(game_name,rst_dir,G_eval,all2all,\n",
    "                                                        all_pairs,task_type=task_name,model_name=model_name,\n",
    "                                                        eval_difficulty=eval_difficulty,eval_set=None)\n",
    "        else:\n",
    "            if eval_dict is not None:\n",
    "                rst=eval_gpt_batch(game_name,rst_dir,G_eval,all2all,\n",
    "                                                    all_pairs,task_type=task_name,model_name=model_name,\n",
    "                                                    eval_difficulty=eval_difficulty,eval_set=eval_dict[game_name])\n",
    "            else:\n",
    "                rst=eval_gpt_batch(game_name,rst_dir,G_eval,all2all,\n",
    "                                                        all_pairs,task_type=task_name,model_name=model_name,\n",
    "                                                        eval_difficulty=eval_difficulty,eval_set=None)\n",
    "        \n",
    "        eval_rst[game_name]=rst\n",
    "        print('-----------------------')\n",
    "        \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    timestamp=get_timetsamp_with_random()\n",
    "    target_name=osp.join(save_path,f'result_{eval_difficulty}_{timestamp}.csv')\n",
    "\n",
    "\n",
    "    get_csv(eval_rst,target_name)\n",
    "\n",
    "    df = pd.read_csv(target_name)\n",
    "    df = df.sort_values('name')\n",
    "    average = df.mean()\n",
    "    \n",
    "    # Convert first column to object type\n",
    "    df[df.columns[0]] = df[df.columns[0]].astype(object)\n",
    "\n",
    "    # Append the average row and set the value of the first column to 'average'\n",
    "    df.loc[len(df)] = ['average'] + list(average.values)\n",
    "\n",
    "    # Replace all NA/NaN in df with 'NA'\n",
    "    df.fillna('NA', inplace=True)\n",
    "    \n",
    "    \n",
    "#     print(average.values)\n",
    "    weighted_average=[]\n",
    "    \n",
    "    weighted_average.append(average.values[6]/average.values[8])\n",
    "    weighted_average.append(average.values[7]/average.values[8])\n",
    "    \n",
    "    weighted_average.append(average.values[12]/average.values[14])\n",
    "    weighted_average.append(average.values[9]/average.values[11])\n",
    "    \n",
    "    weighted_average.append(average.values[13]/average.values[14])\n",
    "    weighted_average.append(average.values[10]/average.values[11])\n",
    "    \n",
    "    for i in range(len(list(average.values))-len(weighted_average)):\n",
    "        weighted_average.append(0.0)\n",
    "\n",
    "    df.loc[len(df)] = ['weighted_average'] + weighted_average\n",
    "    \n",
    "    df.to_csv(f\"{osp.join(save_path,f'sorted_result_{eval_difficulty}_{timestamp}.csv')}\", index=False)\n",
    "    if osp.isfile(target_name):\n",
    "        os.remove(target_name)\n",
    "        print(f'The file \"{target_name}\" has been deleted.')\n",
    "    else:\n",
    "        print(f'The file \"{target_name}\" does not exist.')\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty='strict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983722c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook input\n",
    "rst_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/llama2_13b_base_results_0816'\n",
    "map_dir='/share/data/mei-work/kangrui/github/mango/data/'\n",
    "task_names=[\"stepnav\",\"pathgen\"]\n",
    "model_names=['llama','llama_anno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c55d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rst={}\n",
    "for model_name in model_names:\n",
    "    if model_name not in rst.keys():\n",
    "        rst[model_name]={}\n",
    "    for task_name in task_names:\n",
    "        rst[model_name][task_name]=get_all_task_id(rst_dir,map_dir,model_name,task_name,difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=['gpt-4','gpt-3.5-turbo']\n",
    "rst_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/gpt-games-results-clean-new-new'\n",
    "for model_name in model_names:\n",
    "    if model_name not in rst.keys():\n",
    "        rst[model_name]={}\n",
    "    for task_name in task_names:\n",
    "        rst[model_name][task_name]=get_all_task_id(rst_dir,map_dir,model_name,task_name,difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=['rwkv','rwkv_anno']\n",
    "rst_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/rwkv_results_0709'\n",
    "for model_name in model_names:\n",
    "    if model_name not in rst.keys():\n",
    "        rst[model_name]={}\n",
    "    for task_name in task_names:\n",
    "        rst[model_name][task_name]=get_all_task_id(rst_dir,map_dir,model_name,task_name,difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_intersection_eval(model1,model2,eval_difficulty):\n",
    "    if model1.startswith('llama'):\n",
    "        rst_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/llama2_13b_base_results_0816'\n",
    "    elif model1.startswith('rwkv'):\n",
    "        rst_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/rwkv_results_0709'\n",
    "    else:\n",
    "        rst_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/gpt-games-results-clean-new'\n",
    "    for task_name in task_names:\n",
    "        save_path=osp.join(save_dir,model1,task_name)\n",
    "        eval_all(rst_dir,map_dir,save_path,model1,task_name,eval_difficulty,rst[model2][task_name]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=['llama','llama_anno','gpt-3.5-turbo','gpt-4','rwkv','rwkv_anno']\n",
    "for pair in combinations(model_names,2):\n",
    "    save_dir=f'/share/data/mei-work/kangrui/github/mango/kangrui/eval_results/results_{pair[0]}_vs_{pair[1]}_0806_{difficulty}/'   \n",
    "    excel_intersection_eval(pair[0],pair[1],difficulty)\n",
    "    excel_intersection_eval(pair[1],pair[0],difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in combinations(model_names,2):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de5973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mango",
   "language": "python",
   "name": "mango"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
