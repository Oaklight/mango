{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1f0621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.utils import get_timetsamp_with_random\n",
    "from utils.map_utils import get_game_info_with_G_eval\n",
    "from utils.eval_gpt_utils import get_csv,eval_game\n",
    "import os\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.map_utils import get_game_info_with_G_eval\n",
    "from utils.map_utils_old import find_all_paths\n",
    "from utils.clean_utils import compute_hash_for_path\n",
    "import tiktoken\n",
    "import openai\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from transformers import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8024df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir='/share/data/mei-work/kangrui/github/mango/kangrui/data/gpt-games-results-clean-new-new'\n",
    "map_dir='/share/data/mei-work/kangrui/github/mango/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a3a6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_off_and_walkthrough_text(walkthrough:str,token_size_limit=3600,model_name='gpt-4',cut_off_number=None):\n",
    "    if model_name.startswith('gpt'):\n",
    "        encoder = tiktoken.encoding_for_model(model_name)\n",
    "    elif model_name.startswith(\"llama\"):\n",
    "        encoder = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "        \n",
    "    enc = encoder.encode(walkthrough)\n",
    "    \n",
    "    if len(enc) > token_size_limit:\n",
    "        cut_off_walkthrough_text = encoder.decode(enc[:token_size_limit])\n",
    "    else:\n",
    "        cut_off_walkthrough_text = encoder.decode(enc)\n",
    "        \n",
    "    #print(int(walkthrough.split('NUM: ')[-2].split('\\n')[0]))\n",
    "    \n",
    "    if cut_off_number is None:\n",
    "        cut_off_number = int(cut_off_walkthrough_text.split('NUM: ')[-2].split('\\n')[0])\n",
    "        if cut_off_number > 70:\n",
    "            cut_off_number = 70\n",
    "    else:\n",
    "        cut_off_number=min(int(walkthrough.split('NUM: ')[-2].split('\\n')[0]),cut_off_number)\n",
    "\n",
    "    walkthrough_text = walkthrough.split('NUM: {}'.format(cut_off_number + 1))[0]\n",
    "\n",
    "    return cut_off_number,len(encoder.encode(walkthrough_text))\n",
    "\n",
    "def normalized_edit_distance(s1, s2):\n",
    "    s1 = s1.lower()\n",
    "    s2 = s2.lower()\n",
    "    \n",
    "    s1=s1.split()\n",
    "    s2=s2.split()\n",
    "    \n",
    "    m = len(s1) + 1\n",
    "    n = len(s2) + 1\n",
    "\n",
    "    dp = [[0] * n for _ in range(m)]\n",
    "\n",
    "    for i in range(m):\n",
    "        dp[i][0] = i\n",
    "\n",
    "    for j in range(n):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + 1,  # deletion\n",
    "                    dp[i][j - 1] + 1,  # insertion\n",
    "                    dp[i - 1][j - 1] + 1  # substitution\n",
    "                )\n",
    "    \n",
    "    # Compute the normalized score\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    score = 1 - dp[m - 1][n - 1] / max_len\n",
    "    return score\n",
    "\n",
    "def get_edegs(G):\n",
    "    edges=[]\n",
    "    for edge in G.edges(data=True):\n",
    "        edges.append(edge)\n",
    "    return edges\n",
    "def num_of_locations(G,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    locations=set()\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number:\n",
    "            locations.add(edge[0])\n",
    "            locations.add(edge[1])\n",
    "    return len(locations)\n",
    "\n",
    "def num_of_edges(G,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    cnt=0\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number:\n",
    "            cnt+=1         \n",
    "    return cnt\n",
    "\n",
    "def num_of_exp_edges(G,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    cnt=0\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number and edge[2]['seen_in_forward']:\n",
    "            cnt+=1         \n",
    "    return cnt\n",
    "def num_of_exp_edges(G,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    cnt=0\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number and edge[2]['seen_in_forward']:\n",
    "            cnt+=1         \n",
    "    return cnt\n",
    "\n",
    "def num_of_imp_edges(G,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    cnt=0\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number and (not edge[2]['seen_in_forward']):\n",
    "            cnt+=1         \n",
    "    return cnt\n",
    "\n",
    "def ratio_of_conf_locations(G,all2all,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    locations=set()\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number:\n",
    "            locations.add(edge[0])\n",
    "            locations.add(edge[1])\n",
    "    locations=list(locations)\n",
    "    \n",
    "    score=0\n",
    "    for i in range(len(locations)):\n",
    "        current_max_score = float('-inf')\n",
    "        for j in range(len(locations)):\n",
    "            if i != j:  # Don't compare an item with itself\n",
    "                current_score = normalized_edit_distance(locations[i], locations[j])\n",
    "                if current_score > current_max_score:\n",
    "                    current_max_score = current_score\n",
    "        score+=current_max_score\n",
    "        \n",
    "    return score/len(locations) if len(locations)>0 else 0\n",
    "\n",
    "def num_of_conf_locations(G,all2all,cut_off_number):\n",
    "    edges=get_edegs(G)\n",
    "    locations=set()\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number:\n",
    "            locations.add(edge[0])\n",
    "            locations.add(edge[1])\n",
    "    locations=list(locations)\n",
    "    \n",
    "    score=0\n",
    "    for i in range(len(locations)):\n",
    "        current_max_score = float('-inf')\n",
    "        for j in range(len(locations)):\n",
    "            if i != j:  # Don't compare an item with itself\n",
    "                current_score = normalized_edit_distance(locations[i], locations[j])\n",
    "                if current_score > current_max_score:\n",
    "                    current_max_score = current_score\n",
    "        score+=current_max_score\n",
    "        \n",
    "    return score\n",
    "    \n",
    "def num_of_df_hard(G,all2all,cut_off_number):\n",
    "    idset=set()\n",
    "    cnt=0\n",
    "    for path in all2all:\n",
    "        if path['path_min_cutoff']<=cut_off_number and not path['all_steps_seen_in_forward']:\n",
    "            idset.add(path['id'])\n",
    "            cnt+=1\n",
    "    assert len(idset)==cnt\n",
    "    return cnt\n",
    "\n",
    "def num_of_df_easy(G,all2all,cut_off_number):\n",
    "    idset=set()\n",
    "    cnt=0\n",
    "    for path in all2all:\n",
    "        if path['path_min_cutoff']<=cut_off_number and path['all_steps_seen_in_forward']:\n",
    "            idset.add(path['id'])\n",
    "            cnt+=1\n",
    "    assert len(idset)==cnt\n",
    "    return cnt\n",
    "\n",
    "def is_hard(pair,all2all):\n",
    "    flag=True\n",
    "    found=False\n",
    "    for path in all2all:\n",
    "        if path['src_node']==pair['src_node'] and path['dst_node']==pair['dst_node'] and path['diff_shortest']==0: \n",
    "            found=True\n",
    "            if path['all_steps_seen_in_forward']:\n",
    "                flag=False\n",
    "                break\n",
    "    assert found\n",
    "    return flag\n",
    "                \n",
    "def num_of_rf_hard(G,all_pairs,all2all,cut_off_number):\n",
    "    idset=set()\n",
    "    cnt=0\n",
    "    for pair in all_pairs:\n",
    "        if min(pair['path_min_cutoffs'])<=cut_off_number and is_hard(pair,all2all):\n",
    "            idset.add(pair['id'])\n",
    "            cnt+=1\n",
    "    assert len(idset)==cnt\n",
    "    return cnt\n",
    "\n",
    "def num_of_rf_easy(G,all_pairs,all2all,cut_off_number):\n",
    "    idset=set()\n",
    "    cnt=0\n",
    "    for pair in all_pairs:\n",
    "        if min(pair['path_min_cutoffs'])<=cut_off_number and not is_hard(pair,all2all):\n",
    "            idset.add(pair['id'])\n",
    "            cnt+=1\n",
    "    assert len(idset)==cnt\n",
    "    return cnt\n",
    "\n",
    "def average_length_of_all2all(all2all,cut_off_number):\n",
    "    length=0\n",
    "    cnt=0\n",
    "    for path in all2all:\n",
    "        if path['path_min_cutoff']<=cut_off_number:\n",
    "            length+=path['step_count']\n",
    "            cnt+=1\n",
    "    return length/cnt if cnt>0 else 0\n",
    "\n",
    "def average_length_of_all2all_simple(all2all,cut_off_number):\n",
    "    length=0\n",
    "    cnt=0\n",
    "    for path in all2all:\n",
    "        if path['path_min_cutoff']<=cut_off_number and path['all_steps_seen_in_forward']:\n",
    "            length+=path['step_count']\n",
    "            cnt+=1\n",
    "    return length/cnt if cnt>0 else 0\n",
    "\n",
    "def average_length_of_all2all_hard(all2all,cut_off_number):\n",
    "    length=0\n",
    "    cnt=0\n",
    "    for path in all2all:\n",
    "        if path['path_min_cutoff']<=cut_off_number and not path['all_steps_seen_in_forward']:\n",
    "            length+=path['step_count']\n",
    "            cnt+=1\n",
    "    return length/cnt if cnt>0 else 0\n",
    "        \n",
    "def average_num_of_imp_edge(all2all,cut_off_number):\n",
    "    num=0\n",
    "    cnt=0\n",
    "    for path in all2all:\n",
    "        if path['path_min_cutoff']<=cut_off_number and not path['all_steps_seen_in_forward']:\n",
    "            cnt+=1\n",
    "            for edge in path[\"path_details\"]:\n",
    "                if not edge[\"seen_in_forward\"]:\n",
    "                    num+=1\n",
    "                    \n",
    "    return num/cnt if cnt>0 else 0\n",
    "\n",
    "def num_of_special_moves(G,cut_off_number):\n",
    "    \n",
    "    reverse_dict = {\n",
    "    \"up\": \"down\",\n",
    "    \"down\": \"up\",\n",
    "    \"north\": \"south\",\n",
    "    \"south\": \"north\",\n",
    "    \"east\": \"west\",\n",
    "    \"west\": \"east\",\n",
    "    \"northeast\": \"southwest\",\n",
    "    \"northwest\": \"southeast\",\n",
    "    \"southeast\": \"northwest\",\n",
    "    \"southwest\": \"northeast\"\n",
    "}\n",
    "    \n",
    "    edges=get_edegs(G)\n",
    "    actions=set()\n",
    "    for edge in edges:\n",
    "        if edge[2]['step_min_cutoff']<=cut_off_number:\n",
    "            if edge[2]['action'] not in reverse_dict.keys():\n",
    "                actions.add(edge[2]['action'])\n",
    "    return len(actions)\n",
    "\n",
    "def num_of_tokens_per_edge(G,cut_off_number,token_num):\n",
    "    return token_num/num_of_exp_edges(G,cut_off_number)\n",
    "\n",
    "def analyze_map(map_dir,game_name,cut_off_number=None,token_size_limit=3600,model_name='gpt-4'):\n",
    "    G_eval,G,actions,locations,all2all,all_pairs,walkthrough=get_game_info_with_G_eval(map_dir,game_name)\n",
    "    cut_off_number,token_num=get_cut_off_and_walkthrough_text(walkthrough,cut_off_number=cut_off_number,\n",
    "                                                             token_size_limit=token_size_limit,model_name=model_name)\n",
    "    \n",
    "    result = {\n",
    "        'num_of_loactions':num_of_locations(G,cut_off_number),\n",
    "        'num_of_edges':num_of_edges(G,cut_off_number),\n",
    "        'average_len_of_simple_path':average_length_of_all2all(all2all,cut_off_number),\n",
    "        'num_of_steps':cut_off_number,\n",
    "        'num_of_df_easy':num_of_df_easy(G,all2all,cut_off_number),\n",
    "        'num_of_df_hard':num_of_df_hard(G,all2all,cut_off_number),\n",
    "        'num_of_rf_easy':num_of_rf_easy(G,all_pairs,all2all,cut_off_number),\n",
    "        'num_of_rf_hard':num_of_rf_hard(G,all_pairs,all2all,cut_off_number),\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69f03922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_of_loactions': 19,\n",
       " 'num_of_edges': 34,\n",
       " 'average_len_of_simple_path': 7.156171284634761,\n",
       " 'num_of_steps': 200,\n",
       " 'num_of_df_easy': 351,\n",
       " 'num_of_df_hard': 46,\n",
       " 'num_of_rf_easy': 279,\n",
       " 'num_of_rf_hard': 45}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_name='zork1'\n",
    "analyze_map(map_dir,game_name,cut_off_number=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a1211ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10476 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5969 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11041 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9479 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11560 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8458 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18378 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20437 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10883 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5984 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29122 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15702 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28667 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18148 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8870 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3669 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90380 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5056 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5620 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7129 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18587 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4903 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4694 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21601 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23008 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18954 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22215 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19108 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22704 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30273 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36876 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21755 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (74991 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27632 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8266 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23663 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19875 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19211 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15460 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36338 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52537 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18364 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5791 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22657 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18694 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31649 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26204 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19340 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5971 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5114 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66353 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12353 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_dict1={}\n",
    "for game_name in os.listdir(map_dir):\n",
    "    map_dict1[game_name]=analyze_map(map_dir,game_name,cut_off_number=10000,token_size_limit=1500,model_name='llama')\n",
    "map_dict1 = {key: map_dict1[key] for key in sorted(map_dict1.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d6e45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_total=0\n",
    "df_total=0\n",
    "cnt=0\n",
    "for k,v in map_dict1.items():\n",
    "    rf_total+=v['num_of_rf_easy']\n",
    "    rf_total+=v['num_of_rf_hard']\n",
    "    df_total+=v['num_of_df_easy']\n",
    "    df_total+=v['num_of_df_hard']\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5682a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14698\n",
      "21046\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(rf_total)\n",
    "print(df_total)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8272f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_table_from_dict(data):\n",
    "    for k,v in data.items():\n",
    "        for kk,vv in v.items():\n",
    "            if isinstance(vv,float):\n",
    "                v[kk]=round(float(vv), 2)\n",
    "    \n",
    "    # Extract row and column names from the dictionary\n",
    "    rows = list(data.keys())\n",
    "    columns = list(data[rows[0]].keys())\n",
    "\n",
    "    # Start table construction\n",
    "    latex_code = \"\"\"\n",
    "\\\\begin{table}[htbp]\n",
    "\\\\small\n",
    "\\\\setlength{\\\\tabcolsep}{4pt}\n",
    "\\\\begin{sc}\n",
    "\\\\begin{subtable}{1.0\\\\linewidth}\n",
    "\\\\begin{center}\n",
    "\\\\begin{tabular}{l\"\"\" + \"r\" * len(columns) + \"\"\"}\n",
    "\\\\toprule\n",
    "Method \"\"\" \n",
    "\n",
    "    # Column names\n",
    "    for col in columns:\n",
    "        latex_code += \"& \" + col + \" \"\n",
    "    latex_code += \"\\\\\\\\ \\n\\\\midrule\\n\"\n",
    "\n",
    "    # Row data\n",
    "    for row in rows:\n",
    "        latex_code += row + \" \"\n",
    "        for col in columns:\n",
    "            val = data[row].get(col, \"*\")  # Extract value or set \"*\" if not present\n",
    "            latex_code += \"& \" + str(val) + \" \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "\n",
    "    # End table construction\n",
    "    latex_code += \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{center}\n",
    "\\\\end{subtable}%\n",
    "\\\\end{sc}\n",
    "\\\\caption{Your caption here.}\n",
    "\\\\label{tab:yourlabelhere}\n",
    "\\\\end{table}\n",
    "    \"\"\"\n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c8162e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_code=latex_table_from_dict(map_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ebce760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[htbp]\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{4pt}\n",
      "\\begin{sc}\n",
      "\\begin{subtable}{1.0\\linewidth}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "Method & num_of_loactions & num_of_edges & average_len_of_simple_path & num_of_steps & num_of_df_easy & num_of_df_hard & num_of_rf_easy & num_of_rf_hard \\\\ \n",
      "\\midrule\n",
      "905 & 5 & 7 & 1.88 & 21 & 11 & 5 & 11 & 5 \\\\\n",
      "advent & 31 & 57 & 7.79 & 276 & 692 & 100 & 532 & 100 \\\\\n",
      "adventureland & 18 & 35 & 6.13 & 169 & 579 & 80 & 260 & 46 \\\\\n",
      "afflicted & 11 & 20 & 2.95 & 97 & 100 & 10 & 100 & 10 \\\\\n",
      "anchor & 25 & 46 & 5.99 & 530 & 327 & 153 & 302 & 132 \\\\\n",
      "awaken & 15 & 28 & 5.02 & 56 & 365 & 45 & 171 & 25 \\\\\n",
      "balances & 11 & 18 & 3.09 & 121 & 96 & 8 & 76 & 8 \\\\\n",
      "ballyhoo & 17 & 35 & 4.83 & 415 & 302 & 188 & 213 & 59 \\\\\n",
      "curses & 14 & 27 & 3.62 & 815 & 182 & 13 & 182 & 0 \\\\\n",
      "cutthroat & 25 & 49 & 6.66 & 335 & 471 & 362 & 360 & 216 \\\\\n",
      "deephome & 27 & 49 & 4.83 & 326 & 429 & 19 & 429 & 19 \\\\\n",
      "detective & 32 & 40 & 8.79 & 50 & 505 & 4 & 505 & 4 \\\\\n",
      "dragon & 21 & 44 & 7.11 & 100 & 533 & 990 & 272 & 148 \\\\\n",
      "enchanter & 23 & 43 & 6.35 & 264 & 265 & 219 & 265 & 219 \\\\\n",
      "enter & 14 & 26 & 3.36 & 101 & 117 & 65 & 117 & 65 \\\\\n",
      "gold & 15 & 25 & 3.45 & 344 & 143 & 0 & 143 & 0 \\\\\n",
      "hhgg & 9 & 11 & 2.85 & 360 & 38 & 1 & 38 & 1 \\\\\n",
      "hollywood & 12 & 22 & 3.36 & 396 & 84 & 48 & 84 & 48 \\\\\n",
      "huntdark & 12 & 11 & 4.33 & 66 & 66 & 0 & 66 & 0 \\\\\n",
      "infidel & 24 & 48 & 7.53 & 249 & 312 & 446 & 264 & 288 \\\\\n",
      "inhumane & 30 & 57 & 5.54 & 121 & 614 & 555 & 483 & 280 \\\\\n",
      "jewel & 17 & 32 & 4.4 & 222 & 187 & 85 & 187 & 85 \\\\\n",
      "karn & 19 & 35 & 6.37 & 361 & 339 & 86 & 231 & 63 \\\\\n",
      "library & 7 & 12 & 2.48 & 51 & 42 & 0 & 42 & 0 \\\\\n",
      "loose & 12 & 21 & 4.18 & 49 & 94 & 27 & 94 & 27 \\\\\n",
      "lostpig & 7 & 11 & 2.28 & 145 & 22 & 14 & 22 & 14 \\\\\n",
      "ludicorp & 22 & 43 & 4.91 & 363 & 351 & 111 & 351 & 111 \\\\\n",
      "lurking & 16 & 29 & 4.29 & 293 & 144 & 97 & 143 & 97 \\\\\n",
      "moonlit & 6 & 9 & 2.2 & 58 & 18 & 7 & 18 & 7 \\\\\n",
      "murdac & 30 & 52 & 6.34 & 303 & 537 & 195 & 528 & 183 \\\\\n",
      "night & 20 & 41 & 6.93 & 89 & 633 & 59 & 380 & 0 \\\\\n",
      "omniquest & 29 & 59 & 7.75 & 77 & 536 & 1198 & 290 & 298 \\\\\n",
      "partyfoul & 4 & 9 & 1.97 & 55 & 24 & 6 & 11 & 1 \\\\\n",
      "pentari & 18 & 30 & 3.72 & 48 & 208 & 4 & 208 & 4 \\\\\n",
      "planetfall & 22 & 39 & 5.48 & 398 & 267 & 63 & 267 & 63 \\\\\n",
      "plundered & 22 & 37 & 6.02 & 188 & 450 & 52 & 289 & 26 \\\\\n",
      "reverb & 17 & 31 & 5.26 & 73 & 321 & 20 & 253 & 19 \\\\\n",
      "seastalker & 10 & 15 & 2.7 & 203 & 50 & 3 & 50 & 3 \\\\\n",
      "sherlock & 18 & 28 & 4.36 & 338 & 175 & 5 & 175 & 0 \\\\\n",
      "snacktime & 4 & 6 & 1.5 & 33 & 12 & 0 & 12 & 0 \\\\\n",
      "sorcerer & 26 & 46 & 7.09 & 253 & 340 & 48 & 340 & 48 \\\\\n",
      "spellbrkr & 20 & 31 & 4.84 & 411 & 295 & 23 & 276 & 21 \\\\\n",
      "spirit & 22 & 41 & 4.09 & 1263 & 354 & 87 & 354 & 87 \\\\\n",
      "temple & 19 & 33 & 4.72 & 180 & 178 & 69 & 178 & 69 \\\\\n",
      "trinity & 17 & 17 & 5.96 & 609 & 136 & 1 & 136 & 1 \\\\\n",
      "tryst205 & 9 & 15 & 1.94 & 517 & 64 & 0 & 64 & 0 \\\\\n",
      "wishbringer & 21 & 40 & 6.34 & 183 & 259 & 214 & 251 & 169 \\\\\n",
      "yomomma & 9 & 20 & 2.74 & 97 & 82 & 59 & 43 & 21 \\\\\n",
      "zenon & 14 & 26 & 4.27 & 82 & 96 & 86 & 96 & 86 \\\\\n",
      "zork1 & 19 & 34 & 7.16 & 395 & 351 & 46 & 279 & 45 \\\\\n",
      "zork2 & 22 & 45 & 7.01 & 295 & 536 & 754 & 239 & 130 \\\\\n",
      "zork3 & 23 & 45 & 6.93 & 272 & 627 & 174 & 414 & 70 \\\\\n",
      "ztuu & 15 & 26 & 3.15 & 83 & 183 & 0 & 183 & 0 \\\\\n",
      "\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{subtable}%\n",
      "\\end{sc}\n",
      "\\caption{Your caption here.}\n",
      "\\label{tab:yourlabelhere}\n",
      "\\end{table}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b123e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
