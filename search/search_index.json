{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MANGO!","text":""},{"location":"#whats-mango","title":"What's MANGO?","text":"<p>MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models</p> <p>Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. </p> <p>In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as \"How should you go to Attic from West of House?\" and \"Where are we if we go north and east from Cellar?\". </p> <p>Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large language models in performing relevant downstream tasks, such as playing textgames. </p> <p>Our MANGO benchmark will facilitate future research on methods that improve the mapping and navigation capabilities of language models. We host our leaderboard, data, code, and evaluation program here.</p>"},{"location":"#cite-us","title":"Cite Us","text":"<pre><code>@misc{ding2024mango,\n      title={MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models}, \n      author={Peng Ding and Jiading Fang and Peng Li and Kangrui Wang and Xiaochen Zhou and Mo Yu and Jing Li and Matthew R. Walter and Hongyuan Mei},\n      year={2024},\n      eprint={2403.19913},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"contact/","title":"Contact Us","text":"<p>If you have any questions, please contact us at <code>[dingpeng]@@uchicago.edu</code> or <code>[hongyuan]@@ttic.edu</code>.</p>"},{"location":"data/","title":"Data","text":""},{"location":"data/#data-location","title":"Data location","text":"<p>Our data are hosted on Huggingface. We provide access to the following collections:</p> Name Description Purpose data/huggingface A cleaned collection that only contains test-ready releases Good for LLM benchmark data-intermediate/huggingface A full collection with all of our labeling and intermediate files If you are interested in dig deeper into data labeling, or derive further customized version"},{"location":"data/#word-only-vs-wordid","title":"Word-only vs Word+ID","text":"<p>word-only: We have one version where all nodes are labeled by additional descriptive text to distinguish different locations with similar names.</p> <p>word+ID: In addition, we also prepared another version, where nodes are labeled using minimaly fixed Jericho simulator names with randomized id.</p> <p>We primarily rely on the word-only version as benchmark, yet providing word+ID version for diverse benchmark settings.</p>"},{"location":"leaderboard/","title":"Leaderboard","text":"<p>We hold a leaderboard for evaluating model performance on our MANGO benchmark. We measure the success rate on Destination Finding (DF) questions and Route Finding (RF) problems, both with easy and hard settings. The evaluation is performed on all 53 mazes with average accuracy reported.</p>"},{"location":"leaderboard/#submission","title":"Submission","text":"<p>We welcome new submissions to our MANGO benchmark. To submit a new result, please email dingpeng@uchicago.edu with the paper of your method. We will then update the leaderboard and link your paper accordingly.</p>"},{"location":"leaderboard/#benchmark","title":"\ud83c\udfc6 Benchmark \ud83c\udfc6","text":"Rank Model DF (easy) DF (hard) RF (easy) RF (hard) 1 \ud83e\udd47 GPT-4-0613 0.83 0.58 0.55 0.45 2 \ud83e\udd48 Claude-2 0.81 0.45 0.47 0.19 3 \ud83e\udd49 Clause-1 0.72 0.36 0.33 0.11 4 GPT-3.5-turbo-0613 0.57 0.32 0.15 0.03 5 Llama-2 0.41 0.24 0.03 0.00 6 RWKV 0.19 0.20 0.01 0.00 <p>Updated on: 2024-03-03</p>"}]}