{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MANGO","text":""},{"location":"#whats-mango","title":"What's MANGO?","text":"<p>MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models</p> <p>Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks.</p> <p>In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as \"How should you go to Attic from West of House?\" and \"Where are we if we go north and east from Cellar?\".</p> <p>Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large language models in performing relevant downstream tasks, such as playing textgames.</p> <p>Our MANGO benchmark will facilitate future research on methods that improve the mapping and navigation capabilities of language models. We host our leaderboard, data, code, and evaluation program here.</p>"},{"location":"#cite-us","title":"Cite Us","text":"<pre><code>@misc{ding2024mango,\n      title={MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models}, \n      author={Peng Ding and Jiading Fang and Peng Li and Kangrui Wang and Xiaochen Zhou and Mo Yu and Jing Li and Matthew R. Walter and Hongyuan Mei},\n      year={2024},\n      eprint={2403.19913},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"contact/","title":"Contact Us","text":"<p>If you have any questions, please contact us at</p> <ul> <li>Peng Ding <code>[dingpeng]@@uchicago.edu</code></li> <li>Kangrui Wang <code>[kangruiwang.cs]@@gmail.com</code></li> <li>Matthew Walter <code>[mwalter]@@ttic.edu</code></li> <li>Hongyuan Mei <code>[hongyuan]@@ttic.edu</code></li> </ul>"},{"location":"data/","title":"Data","text":""},{"location":"data/#data-location","title":"Data location","text":"<p>Our data are hosted on Huggingface. We provide access to the following collections:</p> Name Description Purpose variations data A cleaned collection that only contains test-ready releases Good for LLM benchmark - data  - *-objid  - *-randid  - *-70steps data-intermediate A full collection with all of our labeling and intermediate files If you are interested in dig deeper into data labeling, or derive further customized version - data-intermediate  - *-objid  - *-randid  - *-70steps <p>note: if your connection to huggingface.co is slow, you can find us on Huggingface mirror</p>"},{"location":"data/#folder-structure","title":"Folder Structure","text":"<p>Each folder inside <code>data</code> contains the cleaned up files used during LLM inference and results evaluations. Here is the tree structure from game <code>data/night</code> .</p> <pre><code>data/night/\n\u251c\u2500\u2500 night.actions.json      # list of mentioned actions\n\u251c\u2500\u2500 night.all2all.json      # all simple paths between any 2 locations\n\u251c\u2500\u2500 night.all_pairs.json    # all connectivity between any 2 locations\n\u251c\u2500\u2500 night.edges.json        # list of all edges\n\u251c\u2500\u2500 night.locations.json    # list of all locations\n\u2514\u2500\u2500 night.walkthrough       # enriched walkthrough exported from Jericho simulator\n</code></pre> <p>Each folder inside <code>data-intermediate</code> contains all intermediate files we used during data annotation and generation. Here is the tree structure from game <code>data-intermediate/night</code> .</p> <pre><code>data-intermediate/night/\n\u251c\u2500\u2500 night.all2all.json      # all simple paths between any 2 nodes\n\u251c\u2500\u2500 night.all_pairs.json    # all connectivity between any 2 nodes \n\u251c\u2500\u2500 night.anno2code.json    # annotation to codename mapping\n\u251c\u2500\u2500 night.code2anno.json    # codename to annotation mapping\n\u251c\u2500\u2500 night.edges.json        # list of all edges\n\u251c\u2500\u2500 night.map.human         # human map derived from human annotation\n\u251c\u2500\u2500 night.map.machine       # machine map derived from exported action sequences\n\u251c\u2500\u2500 night.map.reversed      # reverse map derived from human annotation map\n\u251c\u2500\u2500 night.moves             # list of mentioned actions\n\u251c\u2500\u2500 night.nodes.json        # list of all nodes\n\u251c\u2500\u2500 night.valid_moves.csv   # human annotation\n\u251c\u2500\u2500 night.walkthrough       # enriched walkthrough exported from Jericho simulator\n\u2514\u2500\u2500 night.walkthrough_acts  # action sequences exported from Jericho simulator\n</code></pre>"},{"location":"data/#variations","title":"Variations","text":""},{"location":"data/#70-step-vs-all-step-version","title":"70-step vs all-step version","text":"<p>In our paper, we benchmark using the first 70 steps of the walkthrough from each game. We also provide all-step versions of both <code>data</code> and <code>data-intermediate</code> collection.</p> <ul> <li> <p>70-step <code>data[-intermediate]-70steps.tar.zst</code>: contains the first 70 steps of each walkthrough. If the complete walkthrough is shorter than 70 steps, then all steps are used.</p> </li> <li> <p>All-step <code>data[-intermediate].tar.zst</code>: contains all steps of each walkthrough.</p> </li> </ul>"},{"location":"data/#word-only-wordid","title":"Word-only &amp; Word+ID","text":"<ul> <li> <p>Word-only <code>data[-intermediate].tar.zst</code>: Nodes are annotated by additional descriptive text to distinguish different locations with similar names.</p> </li> <li> <p>Word + Object ID <code>data[-intermediate]-objid.tar.zst</code>:  variation of the word-only version, where nodes are labeled using minimaly fixed names with object id from Jericho simulator.</p> </li> <li> <p>Word + Random ID <code>data[-intermediate]-randid.tar.zst</code>: variation of the Jericho ID version, where the Jericho object id replaced with randomly generated integer.</p> </li> </ul> <p>We primarily rely on the word-only version as benchmark, yet providing word+ID version for diverse benchmark settings.</p>"},{"location":"data/#how-to-use","title":"How to use","text":"<p>We use <code>data.tar.zst</code> as an example here.</p>"},{"location":"data/#1-download-from-huggingface","title":"1. download from Huggingface","text":""},{"location":"data/#by-directly-download","title":"by directly download","text":""},{"location":"data/#by-git","title":"by git","text":"<p>Make sure you have git-lfs installed</p> <pre><code>git lfs install\ngit clone https://huggingface.co/datasets/mango-ttic/data\n\n# or, use hf-mirror if your connection to huggingface.co is slow\n# git clone https://hf-mirror.com/datasets/mango-ttic/data\n</code></pre>"},{"location":"data/#2-decompress","title":"2. decompress","text":"<p>Because some json files are huge, we use tar.zst to package the data efficiently. </p> <p>You may get <code>zstd</code> from package manager like <code>apt install zstd</code> or <code>dnf install zstd</code> , or using <code>conda install zstd</code> or <code>mamba install zstd</code> , or by using pre-compiled binary distributed on <code>zstd</code> GitHub page.</p> <p>silently decompress</p> <pre><code>tar -I 'zstd -d' -xf data.tar.zst\n</code></pre> <p>or, verbosely decompress</p> <pre><code>zstd -d -c data.tar.zst | tar -xvf -\n</code></pre>"},{"location":"leaderboard/","title":"Leaderboard","text":"<p>We hold a leaderboard for evaluating model performance on our MANGO benchmark. We measure the success rate on Destination Finding (DF) questions and Route Finding (RF) problems, both with easy and hard settings. The evaluation is performed on all 53 mazes with average accuracy reported.</p>"},{"location":"leaderboard/#submission","title":"Submission","text":"<p>We welcome new submissions to our MANGO benchmark. To submit a new result, please email <code>[dingpeng]@@uchicago.edu</code> with the paper of your method. We will then update the leaderboard and link your paper accordingly.</p>"},{"location":"leaderboard/#benchmark","title":"\ud83c\udfc6 Benchmark \ud83c\udfc6","text":"Rank Model DF (easy) DF (hard) RF (easy) RF (hard) 1 \ud83e\udd47 GPT-4-0613 0.83 0.58 0.55 0.45 2 \ud83e\udd48 Claude-2 0.81 0.45 0.47 0.19 3 \ud83e\udd49 Clause-1 0.72 0.36 0.33 0.11 4 GPT-3.5-turbo-0613 0.57 0.32 0.15 0.03 5 Llama-2 0.41 0.24 0.03 0.00 6 RWKV 0.19 0.20 0.01 0.00 <p>Updated on: 2024-03-03</p>"}]}