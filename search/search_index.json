{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MANGO!","text":""},{"location":"#whats-mango","title":"What's MANGO?","text":"<p>MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models</p> <p>Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performances on a variety of natural language processing tasks. In this paper, we take an initiative to investigate their capacities of text-based mapping and navigation. We propose MANGO, a benchmark that includes 53 mazes taken from a suite of textgames. Each maze is paired with a walkthrough that visits all its locations but does \\emph{not} cover all the paths. The evaluation is zero-shot: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as \"how should you go to Attic from West of House?\" and \"where are we if we go north and east from Cellar?\". It turns out that even the best to-date language model GPT-4 fails to give correct answers to a considerable portion of the questions, significantly underperforming humans. What's worse, GPT-4 works poorly on the challenging mazes that are larger and more complex. Our benchmark can be used to track the advances of mapping and navigation abilities of large language models. It will also facilitate future research that develops methods to improve such abilities of language models. We host our data, source code, and evaluation programs at https://mango.ttic.edu.</p>"},{"location":"#data-location","title":"Data location","text":"<p>TODO: links to different sections of Mango</p>"},{"location":"#cite-us","title":"Cite Us","text":"<p>TODO: add bibtex</p>"},{"location":"contact/","title":"Contact Us","text":"<p>If you have any questions, please contact us at <code>[dingpeng]@@uchicago.edu</code> or <code>[hongyuan]@@ttic.edu</code>.</p>"},{"location":"data/","title":"Data","text":""},{"location":"data/#data-location","title":"Data location","text":"<p>TODO: pesumably, we will host the data on a public cloud storage service? (e.g., Google Drive, Huggingface, etc.) TODO: Or we use signup sheet to get the data?</p>"},{"location":"data/#data-format","title":"Data format","text":"<p>TODO: copy the intro part from current github readme</p>"},{"location":"data/#data-statistics","title":"Data statistics","text":"<p>TODO: some stats?</p>"},{"location":"leaderboard/","title":"Leaderboard","text":"<p>We hold a leaderboard for evaluating model performance on our MANGO benchmark. We measure the success rate on Destination Finding (DF) questions and Route Finding (RF) problems, both with easy and hard settings. The evaluation is performed on all 53 mazes with average accuracy reported.</p>"},{"location":"leaderboard/#submission","title":"Submission","text":"<p>We welcome new submissions to our MANGO benchmark. To submit a new result, please email dingpeng@uchicago.edu with the paper of your method. We will then update the leaderboard and link your paper accordingly.</p>"},{"location":"leaderboard/#benchmark","title":"\ud83c\udfc6 Benchmark \ud83c\udfc6","text":"Rank Model DF (easy) DF (hard) RF (easy) RF (hard) 1 \ud83e\udd47 GPT-4-0613 0.83 0.58 0.55 0.45 2 \ud83e\udd48 Claude-2 0.81 0.45 0.47 0.19 3 \ud83e\udd49 Clause-1 0.72 0.36 0.33 0.11 4 GPT-3.5-turbo-0613 0.57 0.32 0.15 0.03 5 Llama-2 0.41 0.24 0.03 0.00 6 RWKV 0.19 0.20 0.01 0.00 <p>Updated on: 2024-03-03</p>"}]}